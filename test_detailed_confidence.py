#!/usr/bin/env python3
"""
Test detailed confidence scores on user's specific examples
"""

import sys
import logging
from pathlib import Path

# Add current directory to path for imports
sys.path.append(str(Path(__file__).parent))

from simple_pure_overfitting import SimplePureOverfittingClassifier
import torch
from sentence_transformers import SentenceTransformer

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def test_detailed_confidence():
    """Test confidence scores on user's specific examples"""
    
    user_examples = [
        "‡¶ú‡¶Æ‡¶ø ‡¶¶‡¶ñ‡¶≤‡ßá‡¶∞ ‡¶ï‡¶ø‡¶≠‡¶æ‡¶¨‡ßá ‡¶™‡ßá‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶ø?",
        "‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶π‡¶æ‡¶∞‡¶ø‡¶Ø‡¶º‡ßá ‡¶Ø‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶¨‡¶á ‡¶™‡ßá‡¶§‡ßá ‡¶ï‡¶ø ‡¶ï‡¶∞‡¶§‡ßá ‡¶π‡¶¨‡ßá?",
        "‡¶π‡¶ú‡ßç‡¶¨  ‡¶ï‡¶∞‡¶§‡ßá ‡¶ö‡¶æ‡¶á, ‡¶ï‡¶ø ‡¶ï‡¶∞‡¶§‡ßá ‡¶π‡¶¨‡ßá?",
        "‡¶Ö‡¶®‡¶≤‡¶æ‡¶á‡¶®‡ßá ‡¶ì‡¶Æ‡¶∞‡¶æ‡¶π‡ßç‚Äå ‡¶®‡¶ø‡¶¨‡¶®‡ßç‡¶ß‡¶® ‡¶ï‡¶∞‡¶§‡ßá ‡¶ï‡¶ø ‡¶ï‡¶∞‡¶§‡ßá ‡¶π‡¶¨‡ßá?",
        "‡¶Æ‡¶ø‡¶â‡¶ü ‡¶ï‡¶∞‡¶§‡ßá ‡¶π‡¶≤‡ßá ‡¶ï‡¶ø ‡¶ï‡¶∞‡¶§‡ßá ‡¶π‡¶¨‡ßá?",
        "‡¶Ü‡¶Æ‡¶ø ‡¶ì‡¶Æ‡¶∞‡¶æ‡¶π‡ßç‚Äå ‡¶ï‡¶∞‡¶¨‡ßã ‡¶ï‡¶ø ‡¶ï‡¶∞‡¶§‡ßá ‡¶π‡¶¨‡ßá?",
        "‡¶ï‡ßã‡¶Æ‡ßç‡¶™‡¶æ‡¶®‡¶ø‡¶§‡ßá ‡¶Ü‡¶¨‡ßá‡¶¶‡¶® ‡¶ï‡¶ø ‡¶®‡¶ø‡¶ú‡ßá ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶ø‡•§",
        "‡¶ú‡¶Æ‡¶ø ‡¶¶‡¶ñ‡¶≤ ‡¶ï‡¶∞‡¶§‡ßá ‡¶ï‡¶ø ‡¶≠‡ßÇ‡¶Æ‡¶ø ‡¶Ö‡¶´‡¶ø‡¶∏‡ßá ‡¶Ø‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶≤‡¶æ‡¶ó‡ßá?",
        "‡¶Ü‡¶Æ‡¶ø ‡¶®‡¶ø‡¶ú‡ßá ‡¶ï‡¶ø ‡¶ö‡¶æ‡¶ï‡¶∞‡¶ø‡¶∞ ‡¶Ü‡¶¨‡ßá‡¶¶‡¶® ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶¨‡ßã?",
        "‡¶ï‡¶æ‡¶∞‡ßã ‡¶∏‡¶æ‡¶π‡¶æ‡¶Ø‡ßç‡¶Ø‡ßá ‡¶ï‡¶ø ‡¶¶‡¶ñ‡¶≤ ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶¨‡ßá?",
        "‡¶ú‡¶Æ‡¶ø ‡¶¶‡¶ñ‡¶≤‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶®‡¶ø‡¶¨‡¶®‡ßç‡¶ß‡¶® ‡¶ï‡¶∞‡¶§‡ßá ‡¶ï‡¶ø ‡¶≤‡¶æ‡¶ó‡ßá?",
        "‡¶ú‡¶®‡ßç‡¶Æ‡¶®‡¶ø‡¶¨‡¶®‡ßç‡¶ß‡¶® ‡¶ï‡¶∞‡¶§‡ßá ‡¶ï‡¶ø ‡¶ï‡¶ø ‡¶¶‡¶∞‡¶ï‡¶æ‡¶∞?",
        "‡¶ú‡¶®‡ßç‡¶Æ‡¶®‡¶ø‡¶¨‡¶®‡ßç‡¶ß‡¶® ‡¶ï‡¶∞‡¶§‡ßá ‡¶ï‡¶ø ‡¶¶‡¶≤‡¶ø‡¶≤ ‡¶≤‡¶æ‡¶ó‡ßá?",
        "‡¶ú‡¶®‡ßç‡¶Æ‡¶®‡¶ø‡¶¨‡¶®‡ßç‡¶ß‡¶® ‡¶ï‡¶∞‡¶§‡ßá ‡¶®‡¶ø‡¶ú‡ßá‡¶∞ ‡¶Æ‡ßã‡¶¨‡¶æ‡¶á‡¶≤ ‡¶•‡¶æ‡¶ï‡¶§‡ßá ‡¶π‡¶¨‡ßá?",
        "‡¶ú‡¶®‡ßç‡¶Æ‡¶®‡¶ø‡¶¨‡¶®‡ßç‡¶ß‡¶® ‡¶ï‡¶∞‡¶§‡ßá ‡¶Æ‡ßã‡¶¨‡¶æ‡¶á‡¶≤ ‡¶®‡¶Æ‡ßç‡¶¨‡¶∞ ‡¶õ‡¶æ‡¶°‡¶º‡¶æ ‡¶Ü‡¶∞ ‡¶ï‡¶ø ‡¶≤‡¶æ‡¶ó‡ßá?",
        "‡¶ú‡¶®‡ßç‡¶Æ‡¶®‡¶ø‡¶¨‡¶®‡ßç‡¶ß‡¶® ‡¶ï‡¶∞‡¶§‡ßá ‡¶è‡¶®‡¶Ü‡¶á‡¶°‡¶ø ‡¶¨‡¶æ ‡¶ú‡¶®‡ßç‡¶Æ‡¶®‡¶ø‡¶¨‡¶®‡ßç‡¶ß‡¶® ‡¶≤‡¶æ‡¶ó‡ßá ‡¶ï‡¶ø?",
        "‡¶π‡¶ú‡ßç‡¶¨ ‡¶Ü‡¶¨‡ßá‡¶¶‡¶® ‡¶®‡¶ø‡¶ú‡ßá ‡¶®‡¶æ ‡¶ï‡¶∞‡¶≤‡ßá ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶®‡¶ø‡¶ß‡¶ø ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º ‡¶ï‡¶ø‡¶®‡¶æ?",
        "‡¶Ü‡¶Æ‡¶ø ‡¶®‡¶ø‡¶ú ‡¶è‡¶≤‡¶æ‡¶ï‡¶æ‡¶∞ ‡¶¨‡¶æ‡¶á‡¶∞‡ßá ‡¶•‡¶æ‡¶ï‡¶ø ‡¶Ö‡¶®‡ßç‡¶Ø ‡¶ï‡ßá‡¶â ‡¶ï‡¶ø ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶®‡¶æ‡¶Æ‡ßá ‡¶ö‡¶æ‡¶Å‡¶¶‡¶æ‡¶¨‡¶æ‡¶ú‡¶ø ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá ‡¶ï‡¶ø‡¶®‡¶æ?",
        "‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶ø‡¶¶‡ßá‡¶∂‡ßá ‡¶•‡¶æ‡¶ï‡¶ø ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶≠‡¶æ‡¶á ‡¶¨‡¶æ ‡¶Ü‡¶§‡ßç‡¶Æ‡ßÄ‡¶Ø‡¶º ‡¶¶‡¶∂ ‡¶™‡¶æ‡¶∞‡¶∏‡ßá‡¶®‡ßç‡¶ü ‡¶Ü‡¶¨‡ßá‡¶¶‡¶® ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá ‡¶ï‡¶ø‡¶®‡¶æ?",
        "‡¶Ü‡¶Æ‡¶ø ‡¶®‡¶æ‡¶∞‡ßÄ ‡¶ú‡¶Æ‡¶ø‡¶∞ ‡¶ï‡¶ø‡¶õ‡ßÅ‡¶á ‡¶¨‡ßÅ‡¶ù‡¶ø ‡¶®‡¶æ, ‡¶Ü‡¶Æ‡¶ø ‡¶∏‡¶ø‡¶®‡ßá‡¶∏‡¶ø‡¶∏ ‡¶è ‡¶Ü‡¶¨‡ßá‡¶¶‡¶® ‡¶ï‡¶∞‡¶æ‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶¨‡ßã ‡¶§‡ßã?",
    ]
    
    # Load classifier
    logger.info("üî¨ DETAILED CONFIDENCE TESTING")
    logger.info("="*80)
    
    classifier = SimplePureOverfittingClassifier()
    classifier.load_all_training_data()
    classifier.create_memory_index()
    
    # Get embeddings for all examples
    model = classifier.model
    example_embeddings = model.encode(user_examples)
    
    print("\nüéØ DETAILED CONFIDENCE ANALYSIS")
    print("="*80)
    
    high_confidence_namjari = []
    medium_confidence_namjari = []
    low_confidence_namjari = []
    
    for i, example in enumerate(user_examples):
        # Get top 5 most similar training examples
        query_embedding = example_embeddings[i:i+1]
        similarities, indices = classifier.faiss_index.search(query_embedding, k=5)
        
        max_similarity = similarities[0][0]
        
        # Get classification
        result = classifier.classify_with_extreme_thresholds(example)
        
        print(f"\nüìù Query: {example}")
        print(f"   üéØ Classification: {result['domain']} (confidence: {result['confidence']:.3f})")
        print(f"   üìä Max Similarity: {max_similarity:.4f}")
        print(f"   üîç Top 5 Similar Training Examples:")
        
        for j in range(5):
            similar_idx = indices[0][j]
            similarity = similarities[0][j]
            similar_text = classifier.training_texts[similar_idx]
            similar_category = classifier.training_labels[similar_idx]
            print(f"      {j+1}. [{similarity:.4f}] ({similar_category}) {similar_text}")
        
        # Categorize by confidence
        if result['domain'] == 'namjari':
            if result['confidence'] > 0.9:
                high_confidence_namjari.append((example, result['confidence'], max_similarity))
            elif result['confidence'] > 0.7:
                medium_confidence_namjari.append((example, result['confidence'], max_similarity))
            else:
                low_confidence_namjari.append((example, result['confidence'], max_similarity))
    
    # Pattern Analysis
    print("\n\nüß† PATTERN ANALYSIS")
    print("="*80)
    
    print(f"\nüî• HIGH Confidence Namjari ({len(high_confidence_namjari)} examples):")
    for example, conf, sim in high_confidence_namjari:
        print(f"   ‚Ä¢ [{conf:.3f}, sim:{sim:.4f}] {example}")
    
    print(f"\nüî∂ MEDIUM Confidence Namjari ({len(medium_confidence_namjari)} examples):")
    for example, conf, sim in medium_confidence_namjari:
        print(f"   ‚Ä¢ [{conf:.3f}, sim:{sim:.4f}] {example}")
    
    print(f"\nüîµ LOW Confidence Namjari ({len(low_confidence_namjari)} examples):")
    for example, conf, sim in low_confidence_namjari:
        print(f"   ‚Ä¢ [{conf:.3f}, sim:{sim:.4f}] {example}")
    
    print(f"\nüìä SIMILARITY DISTRIBUTION:")
    all_sims = [max_similarity for _, _, max_similarity in high_confidence_namjari + medium_confidence_namjari + low_confidence_namjari]
    if all_sims:
        print(f"   üìà Max similarity: {max(all_sims):.4f}")
        print(f"   üìâ Min similarity: {min(all_sims):.4f}")
        print(f"   üìä Avg similarity: {sum(all_sims)/len(all_sims):.4f}")
    
    # Identify patterns
    print(f"\nüéØ PATTERN INSIGHTS:")
    
    # Keywords that appear in queries
    keywords_found = {}
    for example in user_examples:
        words = example.split()
        for word in words:
            if len(word) > 2:  # Skip short words
                keywords_found[word] = keywords_found.get(word, 0) + 1
    
    common_words = sorted(keywords_found.items(), key=lambda x: x[1], reverse=True)[:10]
    print(f"   üìù Most common words:")
    for word, count in common_words:
        print(f"      ‚Ä¢ {word}: {count} times")
    
    # Suggest hyperparameter changes
    print(f"\nüöÄ HYPERPARAMETER SUGGESTIONS FOR EXTREME OVERFITTING:")
    print(f"   1. Lower extreme_threshold to 0.85-0.90 (currently 0.93)")
    print(f"   2. Lower high_threshold to 0.80-0.85 (currently 0.90)")
    print(f"   3. Lower medium_threshold to 0.75-0.80 (currently 0.89)")
    print(f"   4. Use top-3 instead of top-1 similarity")
    print(f"   5. Add weighted averaging of top similarities")
    print(f"   6. Consider context-aware similarity (sentence structure)")
    
    return high_confidence_namjari, medium_confidence_namjari, low_confidence_namjari

if __name__ == "__main__":
    test_detailed_confidence()