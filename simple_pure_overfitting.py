#!/usr/bin/env python3
"""
Simple Pure Overfitting - NO Keywords, NO Training, Just EXTREME Similarity Thresholds
Goal: Use base model with EXTREMELY high thresholds to only accept exact training matches
"""

import pandas as pd
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple
from collections import Counter
import torch
from sentence_transformers import SentenceTransformer
import faiss
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SimplePureOverfittingClassifier:
    """
    Simple pure overfitting without any training
    Strategy: Use base model + EXTREME similarity thresholds to memorize training data
    """
    
    def __init__(self):
        self.model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
        
        # Training data storage
        self.training_texts = []
        self.training_labels = []
        self.training_embeddings = None
        self.faiss_index = None
        
        # EXTREME thresholds for pure overfitting behavior
        self.extreme_threshold = 0.95  # Must be almost identical
        self.high_threshold = 0.92     # Very high similarity
        self.medium_threshold = 0.88   # Still quite high
        
        logger.info("Initialized Simple Pure Overfitting Classifier")
    
    def load_all_training_data(self, data_dir: str = "namjari_questions"):
        """Load ALL training data for memorization"""
        logger.info("Loading ALL training data for memorization...")
        
        data_dir = Path(data_dir)
        all_texts = []
        all_labels = []
        
        csv_files = list(data_dir.glob("*.csv"))
        for csv_file in csv_files:
            df = pd.read_csv(csv_file)
            category = csv_file.stem.replace('namjari_', '')
            
            questions = df['question'].tolist()
            all_texts.extend(questions)
            all_labels.extend([category] * len(questions))
            
            logger.info(f"  {category}: {len(questions)} examples")
        
        self.training_texts = all_texts
        self.training_labels = all_labels
        
        logger.info(f"Total: {len(all_texts)} training examples loaded")
        return all_texts, all_labels
    
    def create_memory_index(self):
        """Create FAISS index to 'memorize' all training data"""
        logger.info("Creating memory index for pure overfitting...")
        
        # Generate embeddings for ALL training data
        self.training_embeddings = self.model.encode(
            self.training_texts, 
            convert_to_tensor=False,
            show_progress_bar=True
        )
        
        # Create exact search index
        dimension = self.training_embeddings.shape[1]
        self.faiss_index = faiss.IndexFlatIP(dimension)
        
        # Normalize for cosine similarity
        embeddings_normalized = self.training_embeddings.astype('float32')
        faiss.normalize_L2(embeddings_normalized)
        self.faiss_index.add(embeddings_normalized)
        
        logger.info(f"Memory index created with {self.faiss_index.ntotal} training examples")
        return self.faiss_index
    
    def classify_with_extreme_thresholds(self, query: str, k: int = 10) -> Dict:
        """
        Classify using EXTREME similarity thresholds
        NO keywords, NO pattern matching - just pure similarity to memorized data
        """
        
        # Get query embedding
        query_embedding = self.model.encode([query], convert_to_tensor=False)
        query_embedding = query_embedding.astype('float32')
        faiss.normalize_L2(query_embedding)
        
        # Search in memory index
        similarities, indices = self.faiss_index.search(query_embedding, k)
        
        # Get top matches
        results = []
        for i, (sim, idx) in enumerate(zip(similarities[0], indices[0])):
            if idx < len(self.training_texts):
                results.append({
                    'text': self.training_texts[idx],
                    'category': self.training_labels[idx],
                    'similarity': float(sim),
                    'rank': i + 1
                })
        
        if not results:
            return {
                'domain': 'out_of_scope',
                'category': 'unknown',
                'confidence': 0.99,
                'method': 'no_training_matches',
                'reasoning': 'No training data matches found'
            }
        
        best_match = results[0]
        top_similarity = best_match['similarity']
        
        # EXTREME threshold checking (NO other logic)
        if top_similarity >= self.extreme_threshold:
            # Almost identical to training data
            top_categories = [r['category'] for r in results[:3]]
            category_counts = Counter(top_categories)
            most_common_category = category_counts.most_common(1)[0][0]
            consensus = category_counts.most_common(1)[0][1] / len(top_categories[:3])
            
            return {
                'domain': 'namjari',
                'category': most_common_category,
                'confidence': min(0.99, top_similarity * consensus),
                'method': 'extreme_similarity',
                'reasoning': f'EXTREME similarity ({top_similarity:.3f}) to training data - almost identical',
                'top_similarity': top_similarity,
                'threshold_used': self.extreme_threshold,
                'consensus_strength': consensus,
                'top_matches': results[:3]
            }
        
        elif top_similarity >= self.high_threshold:
            # Very high similarity
            top_categories = [r['category'] for r in results[:5]]
            category_counts = Counter(top_categories)
            most_common_category = category_counts.most_common(1)[0][0]
            
            return {
                'domain': 'namjari',
                'category': most_common_category,
                'confidence': min(0.90, top_similarity * 0.95),
                'method': 'high_similarity',
                'reasoning': f'High similarity ({top_similarity:.3f}) to training data',
                'top_similarity': top_similarity,
                'threshold_used': self.high_threshold,
                'top_matches': results[:3]
            }
        
        elif top_similarity >= self.medium_threshold:
            # Medium similarity - be very cautious
            top_categories = [r['category'] for r in results[:5]]
            category_counts = Counter(top_categories)
            most_common_category = category_counts.most_common(1)[0][0]
            
            # Check if top categories have strong consensus
            if category_counts.most_common(1)[0][1] >= 3:  # At least 3 out of 5 agree
                return {
                    'domain': 'namjari', 
                    'category': most_common_category,
                    'confidence': min(0.75, top_similarity * 0.85),
                    'method': 'medium_similarity_with_consensus',
                    'reasoning': f'Medium similarity ({top_similarity:.3f}) with category consensus',
                    'top_similarity': top_similarity,
                    'threshold_used': self.medium_threshold,
                    'top_matches': results[:3]
                }
            else:
                # No consensus - reject
                return {
                    'domain': 'out_of_scope',
                    'category': 'unknown',
                    'confidence': min(0.85, 1.0 - top_similarity),
                    'method': 'medium_similarity_no_consensus',
                    'reasoning': f'Medium similarity ({top_similarity:.3f}) but no category consensus - REJECTED',
                    'top_similarity': top_similarity,
                    'threshold_used': self.medium_threshold,
                    'top_matches': results[:3]
                }
        
        else:
            # Low similarity - definitely reject
            return {
                'domain': 'out_of_scope',
                'category': 'unknown',
                'confidence': min(0.98, 1.0 - top_similarity),
                'method': 'low_similarity_rejection',
                'reasoning': f'Low similarity ({top_similarity:.3f}) to all training data - REJECTED',
                'top_similarity': top_similarity,
                'threshold_used': self.medium_threshold,
                'top_matches': results[:3]
            }
    
    def optimize_extreme_thresholds(self, validation_queries: List[Tuple[str, str]]):
        """Optimize thresholds for maximum rejection of out-of-scope"""
        logger.info("Optimizing EXTREME thresholds...")
        
        # Test different threshold combinations
        extreme_thresholds = [0.93, 0.94, 0.95, 0.96, 0.97]
        high_thresholds = [0.90, 0.91, 0.92, 0.93, 0.94]
        medium_thresholds = [0.85, 0.86, 0.87, 0.88, 0.89]
        
        best_accuracy = 0.0
        best_extreme = self.extreme_threshold
        best_high = self.high_threshold
        best_medium = self.medium_threshold
        
        for extreme in extreme_thresholds:
            for high in high_thresholds:
                for medium in medium_thresholds:
                    if not (medium < high < extreme):
                        continue
                    
                    # Test this combination
                    self.extreme_threshold = extreme
                    self.high_threshold = high
                    self.medium_threshold = medium
                    
                    correct = 0
                    total = 0
                    
                    for query, expected in validation_queries:
                        if expected == "ambiguous":
                            continue
                            
                        result = self.classify_with_extreme_thresholds(query)
                        predicted = result['domain']
                        
                        total += 1
                        if predicted == expected:
                            correct += 1
                    
                    accuracy = correct / total if total > 0 else 0.0
                    
                    if accuracy > best_accuracy:
                        best_accuracy = accuracy
                        best_extreme = extreme
                        best_high = high
                        best_medium = medium
        
        # Set best thresholds
        self.extreme_threshold = best_extreme
        self.high_threshold = best_high
        self.medium_threshold = best_medium
        
        logger.info(f"Optimized thresholds:")
        logger.info(f"  Extreme: {best_extreme:.3f}")
        logger.info(f"  High: {best_high:.3f}")
        logger.info(f"  Medium: {best_medium:.3f}")
        logger.info(f"  Best accuracy: {best_accuracy:.3f}")
        
        return best_extreme, best_high, best_medium, best_accuracy


def test_simple_pure_overfitting():
    """Test simple pure overfitting approach"""
    logger.info("ðŸš€ TESTING SIMPLE PURE OVERFITTING (NO KEYWORDS, NO TRAINING)")
    logger.info("="*80)
    
    # Initialize classifier  
    classifier = SimplePureOverfittingClassifier()
    
    # Load all training data
    classifier.load_all_training_data()
    
    # Create memory index (no training, just memorization)
    classifier.create_memory_index()
    
    # Test queries from user
    test_queries = [
        ("à¦†à¦®à¦¿ à¦•à§€à¦­à¦¾à¦¬à§‡ à¦œà¦®à¦¿à¦° à¦¦à¦–à¦² à¦ªà§‡à¦¤à§‡ à¦ªà¦¾à¦°à¦¿?", "out_of_scope"),
        ("à¦†à¦®à¦¾à¦° à¦¹à¦¾à¦°à¦¿à¦¯à¦¼à§‡ à¦¯à¦¾à¦“à¦¯à¦¼à¦¾ à¦¬à¦‡à¦Ÿà¦¿ à¦«à§‡à¦°à¦¤ à¦ªà§‡à¦¤à§‡ à¦•à§€ à¦•à¦°à¦¤à§‡ à¦¹à¦¬à§‡?", "out_of_scope"),
        ("à¦†à¦®à¦¿ à¦¹à¦œà§à¦¬ à¦•à¦°à¦¤à§‡ à¦‡à¦šà§à¦›à§à¦•, à¦†à¦®à¦¾à¦° à¦•à¦°à¦£à§€à¦¯à¦¼ à¦•à§€?", "out_of_scope"),
        ("à¦…à¦¨à¦²à¦¾à¦‡à¦¨à§‡ à¦“à¦®à¦°à¦¾à¦¹à§â€Œ à¦¨à¦¿à¦¬à¦¨à§à¦§à¦¨à§‡à¦° à¦ªà¦¦à§à¦§à¦¤à¦¿ à¦•à§€?", "out_of_scope"),
        ("à¦•à§‹à¦¨à§‹ à¦•à¦¿à¦›à§ à¦®à¦¿à¦‰à¦Ÿ à¦•à¦°à¦¾à¦° à¦‰à¦ªà¦¾à¦¯à¦¼ à¦•à§€?", "out_of_scope"),
        ("à¦šà¦¾à¦•à¦°à¦¿à¦° à¦œà¦¨à§à¦¯ à¦•à¦¿ à¦†à¦®à¦¿ à¦¨à¦¿à¦œà§‡ à¦†à¦¬à§‡à¦¦à¦¨ à¦•à¦°à¦¤à§‡ à¦ªà¦¾à¦°à¦¬à§‹?", "out_of_scope"),
        ("à¦œà¦¨à§à¦® à¦¨à¦¿à¦¬à¦¨à§à¦§à¦¨ à¦•à¦°à¦¾à¦° à¦œà¦¨à§à¦¯ à¦•à§€ à¦•à§€ à¦ªà§à¦°à¦¯à¦¼à§‹à¦œà¦¨?", "out_of_scope"),
        ("à¦¹à¦œà§à¦¬à§‡à¦° à¦†à¦¬à§‡à¦¦à¦¨ à¦•à¦¿ à¦ªà§à¦°à¦¤à¦¿à¦¨à¦¿à¦§à¦¿à¦° à¦®à¦¾à¦§à§à¦¯à¦®à§‡ à¦•à¦°à¦¾ à¦¸à¦®à§à¦­à¦¬?", "out_of_scope"),
        ("à¦†à¦®à¦¿ à¦ªà§à¦°à¦¬à¦¾à¦¸à§€, à¦†à¦®à¦¾à¦° à¦ªà¦•à§à¦· à¦¥à§‡à¦•à§‡ à¦†à¦®à¦¾à¦° à¦­à¦¾à¦‡ à¦¬à¦¾ à¦•à§‹à¦¨à§‹ à¦†à¦¤à§à¦®à§€à¦¯à¦¼ à¦•à¦¿ à§§à§¦ à¦¶à¦¤à¦¾à¦‚à¦¶ à¦•à§‹à¦Ÿà¦¾à¦¯à¦¼ à¦†à¦¬à§‡à¦¦à¦¨ à¦•à¦°à¦¤à§‡ à¦ªà¦¾à¦°à¦¬à§‡?", "out_of_scope"),
        
        # Some actual training examples
        ("à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦°à¦¤à§‡ à¦•à¦¿ à¦•à¦°à¦¤à§‡ à¦¹à¦¬à§‡?", "namjari"),
        ("à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿à¦° à¦«à¦¿ à¦•à¦¤?", "namjari"),
        ("à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦°à¦¤à§‡ à¦•à¦¿ à¦¦à¦²à¦¿à¦² à¦²à¦¾à¦—à§‡?", "namjari"),
        ("à¦–à¦¤à¦¿à¦¯à¦¼à¦¾à¦¨à§‡à¦° à¦•à¦ªà¦¿ à¦•à¦¿à¦­à¦¾à¦¬à§‡ à¦ªà¦¾à¦¬à§‹?", "namjari"),
        
        # Slight variations of training examples
        ("à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿ à¦•à¦¿à¦­à¦¾à¦¬à§‡ à¦•à¦°à¦¤à§‡ à¦¹à¦¬à§‡?", "namjari"),  # Word order change
        ("à¦¨à¦¾à¦®à¦œà¦¾à¦°à¦¿à¦° à¦–à¦°à¦š à¦•à¦¤?", "namjari"),  # Synonym (à¦–à¦°à¦š instead of à¦«à¦¿)
    ]
    
    # Optimize thresholds
    classifier.optimize_extreme_thresholds(test_queries)
    
    # Test all queries
    logger.info("\n" + "="*60)
    logger.info("SIMPLE PURE OVERFITTING RESULTS:")
    logger.info("="*60)
    
    out_of_scope_correct = 0
    out_of_scope_total = 0
    namjari_correct = 0
    namjari_total = 0
    
    for query, expected in test_queries:
        result = classifier.classify_with_extreme_thresholds(query)
        predicted = result['domain']
        
        is_correct = predicted == expected
        
        if expected == "out_of_scope":
            out_of_scope_total += 1
            if is_correct:
                out_of_scope_correct += 1
        elif expected == "namjari":
            namjari_total += 1
            if is_correct:
                namjari_correct += 1
        
        status = "âœ…" if is_correct else "âŒ"
        
        logger.info(f"\n{status} Query: '{query[:55]}{'...' if len(query) > 55 else ''}'")
        logger.info(f"    Expected: {expected}")
        logger.info(f"    Predicted: {predicted} (confidence: {result['confidence']:.3f})")
        logger.info(f"    Method: {result['method']}")
        logger.info(f"    Similarity: {result.get('top_similarity', 0):.3f} (threshold: {result.get('threshold_used', 0):.3f})")
        
        # Show closest training example
        if 'top_matches' in result and result['top_matches']:
            closest = result['top_matches'][0]
            logger.info(f"    Closest: '{closest['text'][:40]}...' ({closest['similarity']:.3f})")
    
    # Calculate metrics
    out_of_scope_accuracy = out_of_scope_correct / out_of_scope_total if out_of_scope_total > 0 else 0
    namjari_accuracy = namjari_correct / namjari_total if namjari_total > 0 else 0
    overall_accuracy = (out_of_scope_correct + namjari_correct) / (out_of_scope_total + namjari_total)
    
    logger.info(f"\n{'='*60}")
    logger.info("SIMPLE PURE OVERFITTING FINAL RESULTS:")
    logger.info(f"{'='*60}")
    logger.info(f"ðŸ“Š Out-of-scope Detection: {out_of_scope_correct}/{out_of_scope_total} ({out_of_scope_accuracy*100:.1f}%)")
    logger.info(f"ðŸ“Š Namjari Detection: {namjari_correct}/{namjari_total} ({namjari_accuracy*100:.1f}%)")
    logger.info(f"ðŸ“Š Overall Accuracy: {overall_accuracy*100:.1f}%")
    logger.info(f"ðŸŽ¯ Extreme Threshold: {classifier.extreme_threshold:.3f}")
    logger.info(f"ðŸŽ¯ High Threshold: {classifier.high_threshold:.3f}")
    logger.info(f"ðŸŽ¯ Medium Threshold: {classifier.medium_threshold:.3f}")
    
    return classifier, {
        'out_of_scope_accuracy': out_of_scope_accuracy,
        'namjari_accuracy': namjari_accuracy,
        'overall_accuracy': overall_accuracy,
        'extreme_threshold': classifier.extreme_threshold,
        'high_threshold': classifier.high_threshold,
        'medium_threshold': classifier.medium_threshold
    }


if __name__ == "__main__":
    try:
        classifier, results = test_simple_pure_overfitting()
        
        print("\n" + "="*80)
        print("ðŸ† SIMPLE PURE OVERFITTING RESULTS")
        print("="*80)
        print(f"ðŸ“Š Out-of-scope Detection: {results['out_of_scope_accuracy']*100:.1f}%")
        print(f"ðŸ“Š Namjari Detection: {results['namjari_accuracy']*100:.1f}%")
        print(f"ðŸ“Š Overall Accuracy: {results['overall_accuracy']*100:.1f}%")
        print(f"ðŸŽ¯ Optimized Thresholds:")
        print(f"    Extreme: {results['extreme_threshold']:.3f}")
        print(f"    High: {results['high_threshold']:.3f}")
        print(f"    Medium: {results['medium_threshold']:.3f}")
        
        print("\nðŸš€ Pure Overfitting Strategy:")
        print("   1. âœ… NO keyword patterns whatsoever")
        print("   2. âœ… NO model fine-tuning or training")
        print("   3. âœ… Base model + EXTREME similarity thresholds")
        print("   4. âœ… Memorizes all 998 training examples via FAISS")
        print("   5. âœ… Only accepts queries VERY similar to training data")
        
        print("\nðŸ’¡ How it works:")
        print("   â€¢ Loads all training data into memory (FAISS index)")
        print("   â€¢ For each query, finds most similar training examples")
        print("   â€¢ Only accepts if similarity > 0.95 (EXTREME threshold)")
        print("   â€¢ Everything else gets rejected as out-of-scope")
        print("   â€¢ NO keywords, NO patterns - pure embedding similarity")
        
        print("="*80)
        
    except Exception as e:
        logger.error(f"Simple pure overfitting failed: {e}")
        raise